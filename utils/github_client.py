import base64
import random
import time
import traceback
from typing import Dict, List, Optional, Any
from urllib.parse import urlencode, quote

import requests
from bs4 import BeautifulSoup

from common.Logger import logger
from common.config import Config
from common.translations import get_translator

# è·å–ç¿»è¯‘å‡½æ•°
t = get_translator().t


class GitHubClient:
    GITHUB_API_URL = "https://api.github.com/search/code"
    GITHUB_WEB_SEARCH_URL = "https://github.com/search"

    def __init__(self, tokens: List[str], auth_mode: str = 'token', github_sessions: List[str] = None):
        """
        åˆå§‹åŒ–GitHubå®¢æˆ·ç«¯
        
        Args:
            tokens: GitHub Tokenåˆ—è¡¨ï¼ˆtokenæ¨¡å¼ä½¿ç”¨ï¼‰
            auth_mode: è®¤è¯æ¨¡å¼ï¼Œ'token' æˆ– 'web'
            github_sessions: GitHubçš„user_session cookieåˆ—è¡¨ï¼ˆwebæ¨¡å¼ä½¿ç”¨ï¼Œæ”¯æŒå¤šä¸ªè½®è¯¢ï¼‰
        """
        self.auth_mode = auth_mode.lower()
        self.tokens = [token.strip() for token in tokens if token.strip()]
        self._token_ptr = 0
        
        # å¤„ç†sessionså‚æ•°ï¼ˆå…¼å®¹æ—§çš„å­—ç¬¦ä¸²å‚æ•°å’Œæ–°çš„åˆ—è¡¨å‚æ•°ï¼‰
        if github_sessions is None:
            github_sessions = []
        elif isinstance(github_sessions, str):
            # å…¼å®¹æ—§çš„å­—ç¬¦ä¸²å‚æ•°
            github_sessions = [github_sessions.strip()] if github_sessions.strip() else []
        else:
            github_sessions = [s.strip() for s in github_sessions if s.strip()]
        
        self.github_sessions = github_sessions
        self._session_ptr = 0
        
        # Webæ¨¡å¼ï¼šåˆ›å»ºsessionå¯¹è±¡
        if self.auth_mode == 'web':
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
            })
            
            if self.github_sessions:
                logger.info(f"ğŸŒ GitHubå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼ˆWebæ¨¡å¼ï¼Œ{len(self.github_sessions)} ä¸ªsession cookieï¼‰")
            else:
                logger.warning("âš ï¸ Webæ¨¡å¼æœªæä¾›user_session cookieï¼Œå¯èƒ½ä¼šå—åˆ°æ›´å¤šé™åˆ¶")
        else:
            self.session = None
            logger.info(f"ğŸ”‘ GitHubå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼ˆTokenæ¨¡å¼ï¼Œ{len(self.tokens)} ä¸ªtokenï¼‰")

    def _next_token(self) -> Optional[str]:
        """è·å–ä¸‹ä¸€ä¸ªtokenï¼ˆè½®è¯¢ï¼‰"""
        if not self.tokens:
            return None

        token = self.tokens[self._token_ptr % len(self.tokens)]
        self._token_ptr += 1

        return token.strip() if isinstance(token, str) else token
    
    def _next_session(self) -> Optional[str]:
        """è·å–ä¸‹ä¸€ä¸ªsession cookieï¼ˆè½®è¯¢ï¼‰"""
        if not self.github_sessions:
            return None
        
        session_cookie = self.github_sessions[self._session_ptr % len(self.github_sessions)]
        self._session_ptr += 1
        
        return session_cookie.strip() if isinstance(session_cookie, str) else session_cookie
    
    def _set_session_cookie(self, session_cookie: str = None):
        """è®¾ç½®å½“å‰sessionçš„cookie"""
        if not self.session:
            return
        
        # æ¸…é™¤ç°æœ‰çš„ user_session cookie
        self.session.cookies.set('user_session', '', domain='.github.com', path='/')
        
        # è®¾ç½®æ–°çš„ cookie
        if session_cookie:
            self.session.cookies.set('user_session', session_cookie, domain='.github.com', path='/')

    def search_for_keys(self, query: str, max_retries: int = 8) -> Dict[str, Any]:
        """
        æœç´¢å¯†é’¥ï¼Œæ ¹æ®auth_modeé€‰æ‹©APIæˆ–Webæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            åŒ…å«æœç´¢ç»“æœçš„å­—å…¸
        """
        if self.auth_mode == 'web':
            return self._search_web(query, max_retries)
        else:
            return self._search_api(query, max_retries)
    
    def _search_api(self, query: str, max_retries: int = 8) -> Dict[str, Any]:
        """ä½¿ç”¨GitHub APIè¿›è¡Œæœç´¢ï¼ˆéœ€è¦tokenï¼‰"""
        all_items = []
        total_count = 0
        expected_total = None
        pages_processed = 0

        # ç»Ÿè®¡ä¿¡æ¯
        total_requests = 0
        failed_requests = 0
        rate_limit_hits = 0
        failed_pages = []  # è®°å½•å¤±è´¥çš„é¡µç 

        for page in range(1, 11):
            page_result = None
            page_success = False

            for attempt in range(1, max_retries + 1):
                current_token = self._next_token()

                headers = {
                    "Accept": "application/vnd.github.v3+json",
                    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
                }

                if current_token:
                    current_token = current_token.strip()
                    headers["Authorization"] = f"token {current_token}"

                params = {
                    "q": query,
                    "per_page": 100,
                    "page": page
                }

                try:
                    total_requests += 1
                    # è·å–éšæœºproxyé…ç½®
                    proxies = Config.get_random_proxy()
                    
                    if proxies:
                        response = requests.get(self.GITHUB_API_URL, headers=headers, params=params, timeout=30, proxies=proxies)
                    else:
                        response = requests.get(self.GITHUB_API_URL, headers=headers, params=params, timeout=30)
                    rate_limit_remaining = response.headers.get('X-RateLimit-Remaining')
                    # åªåœ¨å‰©ä½™æ¬¡æ•°å¾ˆå°‘æ—¶è­¦å‘Š
                    if rate_limit_remaining and int(rate_limit_remaining) < 3:
                        logger.warning(t('rate_limit_low', rate_limit_remaining, current_token))
                    response.raise_for_status()
                    page_result = response.json()
                    
                    page_success = True
                    break

                except requests.exceptions.HTTPError as e:
                    status = e.response.status_code if e.response else None
                    failed_requests += 1
                    
                    # è·å–tokenæ˜¾ç¤ºï¼ˆè„±æ•å¤„ç†ï¼‰
                    token_display = current_token[:20] if current_token else "None"
                    
                    # å°è¯•ä»å“åº”ä¸­æå–è¯¦ç»†é”™è¯¯ä¿¡æ¯
                    error_message = "Unknown error"
                    try:
                        if e.response is not None:
                            error_json = e.response.json()
                            error_message = error_json.get('message', str(e))
                        else:
                            error_message = str(e)
                    except:
                        error_message = str(e)
                    
                    # æ ¹æ®ä¸åŒçš„çŠ¶æ€ç æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                    if status == 401:
                        # Token æ— æ•ˆ
                        logger.error(t('token_invalid', token_display, error_message))
                        time.sleep(2 ** attempt)
                        continue
                    elif status == 403:
                        # Token è¢«ç¦æ­¢æˆ–æƒé™ä¸è¶³ï¼Œå¯èƒ½æ˜¯é€Ÿç‡é™åˆ¶
                        rate_limit_hits += 1
                        rate_limit_remaining = e.response.headers.get('X-RateLimit-Remaining', 'N/A')
                        rate_limit_reset = e.response.headers.get('X-RateLimit-Reset', 'N/A')
                        
                        # è½¬æ¢é‡ç½®æ—¶é—´ä¸ºå¯è¯»æ ¼å¼
                        if rate_limit_reset != 'N/A':
                            try:
                                from datetime import datetime
                                reset_time = datetime.fromtimestamp(int(rate_limit_reset)).strftime('%Y-%m-%d %H:%M:%S')
                            except:
                                reset_time = rate_limit_reset
                        else:
                            reset_time = 'N/A'
                        
                        # åˆ¤æ–­æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶
                        if 'rate limit' in error_message.lower() or rate_limit_remaining == '0':
                            logger.warning(t('token_rate_limited', token_display, rate_limit_remaining, reset_time))
                        else:
                            logger.error(t('token_forbidden', token_display, error_message))
                        
                        wait = min(2 ** attempt + random.uniform(0, 1), 60)
                        if attempt >= 3:
                            logger.warning(t('rate_limit_hit', status, attempt, max_retries, wait))
                        time.sleep(wait)
                        continue
                    elif status == 422:
                        # æŸ¥è¯¢è¯­æ³•é”™è¯¯ï¼ˆUnprocessable Entityï¼‰
                        logger.error(t('query_syntax_error', query[:80], error_message))
                        # æŸ¥è¯¢è¯­æ³•é”™è¯¯ä¸éœ€è¦é‡è¯•ï¼Œè¿”å›ç‰¹æ®Šæ ‡è®°
                        return {"items": [], "total_count": 0, "query_syntax_error": True}
                    elif status == 429:
                        # æ˜ç¡®çš„é€Ÿç‡é™åˆ¶
                        rate_limit_hits += 1
                        rate_limit_remaining = e.response.headers.get('X-RateLimit-Remaining', '0')
                        rate_limit_reset = e.response.headers.get('X-RateLimit-Reset', 'N/A')
                        
                        # è½¬æ¢é‡ç½®æ—¶é—´
                        if rate_limit_reset != 'N/A':
                            try:
                                from datetime import datetime
                                reset_time = datetime.fromtimestamp(int(rate_limit_reset)).strftime('%Y-%m-%d %H:%M:%S')
                            except:
                                reset_time = rate_limit_reset
                        else:
                            reset_time = 'N/A'
                        
                        logger.warning(t('token_rate_limited', token_display, rate_limit_remaining, reset_time))
                        wait = min(2 ** attempt + random.uniform(0, 1), 60)
                        time.sleep(wait)
                        continue
                    else:
                        # å…¶ä»–HTTPé”™è¯¯
                        if attempt == max_retries:
                            logger.error(t('token_error_detail', status or 'None', token_display, error_message))
                        time.sleep(2 ** attempt)
                        continue

                except requests.exceptions.RequestException as e:
                    failed_requests += 1
                    wait = min(2 ** attempt, 30)

                    # åªåœ¨æœ€åä¸€æ¬¡å°è¯•æ—¶è®°å½•ç½‘ç»œé”™è¯¯
                    if attempt == max_retries:
                        logger.error(t('network_error', max_retries, page, type(e).__name__))

                    time.sleep(wait)
                    continue

            if not page_success or not page_result:
                if page == 1:
                    # ç¬¬ä¸€é¡µå¤±è´¥æ˜¯ä¸¥é‡é—®é¢˜
                    logger.error(t('first_page_failed', query[:50]))
                    break
                # è®°å½•å¤±è´¥é¡µé¢ä¿¡æ¯ï¼Œä¾¿äºè¯Šæ–­
                failed_pages.append(page)
                logger.warning(f"âš ï¸ ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥ï¼Œå·²è·³è¿‡ï¼ˆå¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±ï¼‰")
                continue

            pages_processed += 1

            if page == 1:
                total_count = page_result.get("total_count", 0)
                expected_total = min(total_count, 1000)
                
                if total_count > 0:
                    logger.info(f"   ğŸ”¢ GitHubè¿”å›æ€»æ•°: {total_count} (é¢„æœŸè·å–: {expected_total})")

            items = page_result.get("items", [])
            current_page_count = len(items)

            if current_page_count == 0:
                if expected_total and len(all_items) < expected_total:
                    continue
                else:
                    break

            all_items.extend(items)

            if expected_total and len(all_items) >= expected_total:
                break

            if page < 10:
                sleep_time = random.uniform(0.5, 1.5)
                logger.info(t('processing_query', query, page, current_page_count, expected_total, total_count, sleep_time))
                time.sleep(sleep_time)

        final_count = len(all_items)

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if expected_total and final_count < expected_total:
            discrepancy = expected_total - final_count
            if discrepancy > expected_total * 0.1:  # è¶…è¿‡10%æ•°æ®ä¸¢å¤±
                warning_msg = t('data_loss_warning', discrepancy, expected_total, discrepancy / expected_total * 100)
                if failed_pages:
                    warning_msg += f" | å¤±è´¥é¡µé¢: {failed_pages}"
                logger.warning(warning_msg)

        # ä¸»è¦æˆåŠŸæ—¥å¿— - ä¸€æ¡æ—¥å¿—åŒ…å«æ‰€æœ‰å…³é”®ä¿¡æ¯
        logger.info(t('search_complete', query, pages_processed, final_count, expected_total or '?', total_requests))

        result = {
            "total_count": total_count,
            "incomplete_results": final_count < expected_total if expected_total else False,
            "items": all_items
        }

        return result
    
    def _search_web(self, query: str, max_retries: int = 8) -> Dict[str, Any]:
        """ä½¿ç”¨Webæ–¹å¼æœç´¢ï¼ˆåŸºäºcookieè®¤è¯ï¼‰"""
        all_items = []
        total_count = 0
        expected_total = None
        pages_processed = 0
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_requests = 0
        failed_requests = 0
        failed_pages = []
        
        logger.info(f"ğŸŒ ä½¿ç”¨Webæ¨¡å¼æœç´¢: {query[:50]}...")
        
        for page in range(1, 11):
            page_success = False
            
            # æ¯ä¸ªæ–°é¡µé¢è½®æ¢ä¸€æ¬¡session cookieï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼‰
            if self.github_sessions and len(self.github_sessions) > 1:
                current_session = self._next_session()
                self._set_session_cookie(current_session)
            
            for attempt in range(1, max_retries + 1):
                try:
                    total_requests += 1
                    
                    # æ„å»ºæœç´¢URL
                    params = {
                        'q': query,
                        'type': 'code',
                        'p': page
                    }
                    
                    # è·å–éšæœºproxyé…ç½®
                    proxies = Config.get_random_proxy()
                    
                    # å‘é€è¯·æ±‚
                    if proxies:
                        response = self.session.get(
                            self.GITHUB_WEB_SEARCH_URL,
                            params=params,
                            timeout=30,
                            proxies=proxies,
                            allow_redirects=True
                        )
                    else:
                        response = self.session.get(
                            self.GITHUB_WEB_SEARCH_URL,
                            params=params,
                            timeout=30,
                            allow_redirects=True
                        )
                    
                    response.raise_for_status()
                    
                    # è§£æHTML
                    items = self._parse_search_results(response.text)
                    
                    if page == 1:
                        # ä»ç¬¬ä¸€é¡µä¼°ç®—æ€»æ•°
                        total_count = self._estimate_total_count(response.text)
                        expected_total = min(total_count, 1000)
                        
                        if total_count > 0:
                            logger.info(f"   ğŸ”¢ Webæœç´¢ä¼°ç®—æ€»æ•°: {total_count} (é¢„æœŸè·å–: {expected_total})")
                    
                    current_page_count = len(items)
                    
                    if current_page_count == 0:
                        if expected_total and len(all_items) < expected_total:
                            # å¯èƒ½è¢«GitHubé™åˆ¶äº†ï¼Œå°è¯•å»¶é•¿ç­‰å¾…
                            wait_time = min(2 ** attempt, 30)
                            logger.warning(f"âš ï¸ ç¬¬ {page} é¡µè¿”å›0ç»“æœï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                            time.sleep(wait_time)
                            continue
                        else:
                            page_success = True
                            break
                    
                    all_items.extend(items)
                    pages_processed += 1
                    page_success = True
                    
                    break
                    
                except requests.exceptions.HTTPError as e:
                    status = e.response.status_code if e.response else None
                    failed_requests += 1
                    
                    if status == 429:
                        # é€Ÿç‡é™åˆ¶
                        wait = min(2 ** attempt + random.uniform(0, 1), 60)
                        logger.warning(f"â° Webæ¨¡å¼é­é‡é€Ÿç‡é™åˆ¶ (429)ï¼Œç­‰å¾… {wait:.1f} ç§’...")
                        time.sleep(wait)
                        continue
                    elif status == 422:
                        # æŸ¥è¯¢è¯­æ³•é”™è¯¯
                        logger.error(f"âŒ æŸ¥è¯¢è¯­æ³•é”™è¯¯: {query[:80]}")
                        return {"items": [], "total_count": 0, "query_syntax_error": True}
                    else:
                        if attempt == max_retries:
                            logger.error(f"âŒ HTTPé”™è¯¯ {status}: {str(e)}")
                        time.sleep(2 ** attempt)
                        continue
                
                except requests.exceptions.RequestException as e:
                    failed_requests += 1
                    wait = min(2 ** attempt, 30)
                    
                    if attempt == max_retries:
                        logger.error(f"âŒ ç½‘ç»œé”™è¯¯ï¼ˆç¬¬{page}é¡µï¼‰: {type(e).__name__}")
                    
                    time.sleep(wait)
                    continue
            
            if not page_success:
                if page == 1:
                    logger.error(f"âŒ ç¬¬ä¸€é¡µè¯·æ±‚å¤±è´¥: {query[:50]}")
                    break
                failed_pages.append(page)
                logger.warning(f"âš ï¸ ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥ï¼Œå·²è·³è¿‡")
                continue
            
            if expected_total and len(all_items) >= expected_total:
                break
            
            # æ²¡æœ‰æ›´å¤šç»“æœ
            if len(items) == 0:
                break
            
            # é¡µé¢é—´å»¶è¿Ÿ
            if page < 10:
                sleep_time = random.uniform(2, 4)  # Webæ¨¡å¼å»¶è¿Ÿæ›´é•¿
                logger.info(f"   ğŸ“„ ç¬¬{page}é¡µ: {len(items)}ä¸ªç»“æœ | é¢„æœŸæ€»æ•°: {expected_total or '?'} | ç­‰å¾… {sleep_time:.1f}s")
                time.sleep(sleep_time)
        
        final_count = len(all_items)
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if expected_total and final_count < expected_total:
            discrepancy = expected_total - final_count
            if discrepancy > expected_total * 0.1:
                warning_msg = f"âš ï¸ æ•°æ®å¯èƒ½ä¸å®Œæ•´: ç¼ºå°‘ {discrepancy}/{expected_total} ({discrepancy / expected_total * 100:.1f}%)"
                if failed_pages:
                    warning_msg += f" | å¤±è´¥é¡µé¢: {failed_pages}"
                logger.warning(warning_msg)
        
        logger.info(f"âœ… Webæœç´¢å®Œæˆ: {query[:50]}... | é¡µæ•°: {pages_processed} | ç»“æœ: {final_count}/{expected_total or '?'} | è¯·æ±‚: {total_requests}")
        
        result = {
            "total_count": total_count,
            "incomplete_results": final_count < expected_total if expected_total else False,
            "items": all_items
        }
        
        return result
    
    def _parse_search_results(self, html: str) -> List[Dict[str, Any]]:
        """è§£æGitHubæœç´¢ç»“æœHTMLé¡µé¢"""
        items = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # æ–¹æ³•1: é€šè¿‡search-titleæ‰¾åˆ°ç»“æœå®¹å™¨
            # GitHubç°åœ¨ä½¿ç”¨è¿™ç§ç»“æ„
            result_containers = soup.find_all('div', class_=lambda x: x and 'search-title' in x if x else False)
            
            if not result_containers:
                # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥æŸ¥æ‰¾æ‰€æœ‰blobé“¾æ¥
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç»“æœå®¹å™¨ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•æŸ¥æ‰¾blobé“¾æ¥")
                blob_links = soup.find_all('a', href=lambda x: x and '/blob/' in x if x else False)
                
                # è¿‡æ»¤æ‰é‡å¤çš„é“¾æ¥ï¼ˆåŒä¸€ä¸ªæ–‡ä»¶å¯èƒ½æœ‰å¤šä¸ªè¡Œå·é“¾æ¥ï¼‰
                seen_files = set()
                for link in blob_links:
                    href = link.get('href', '')
                    # ç§»é™¤è¡Œå·é”šç‚¹
                    file_url = href.split('#')[0]
                    if file_url not in seen_files:
                        seen_files.add(file_url)
                        result_containers.append(link.parent.parent if link.parent and link.parent.parent else link.parent)
            
            # ç”¨äºå»é‡
            seen_files = set()
            
            for container in result_containers:
                if not container:
                    continue
                    
                try:
                    # åœ¨å®¹å™¨ä¸­æŸ¥æ‰¾blobé“¾æ¥
                    link_elem = container.find('a', href=lambda x: x and '/blob/' in x if x else False)
                    
                    if not link_elem:
                        continue
                    
                    file_url = link_elem.get('href', '')
                    if not file_url:
                        continue
                    
                    # ç§»é™¤è¡Œå·é”šç‚¹ï¼ˆå¦‚ #L274ï¼‰
                    file_url = file_url.split('#')[0]
                    
                    # å»é‡
                    if file_url in seen_files:
                        continue
                    seen_files.add(file_url)
                        
                    if not file_url.startswith('http'):
                        file_url = 'https://github.com' + file_url
                    
                    # ä»URLè§£æä»“åº“å’Œæ–‡ä»¶è·¯å¾„
                    # URLæ ¼å¼: https://github.com/{owner}/{repo}/blob/{branch}/{path}
                    # ä¾‹å¦‚: /Benjamin-Loison/YouTube-operational-API/blob/0d2768a5fcf560288eb3a9fa573056bdd5dba3d2/index.php
                    url_parts = file_url.replace('https://github.com/', '').split('/')
                    
                    if len(url_parts) >= 5 and url_parts[2] == 'blob':
                        owner = url_parts[0]
                        repo = url_parts[1]
                        branch = url_parts[3]
                        # blobå’Œbranchä¹‹åæ˜¯æ–‡ä»¶è·¯å¾„
                        file_path = '/'.join(url_parts[4:])
                        
                        # æ„é€ itemå¯¹è±¡ï¼ˆæ¨¡æ‹ŸAPIè¿”å›æ ¼å¼ï¼‰
                        item = {
                            'name': url_parts[-1] if url_parts else '',
                            'path': file_path,
                            'sha': f"web_{hash(file_url) & 0xFFFFFFFF:08x}",  # ç”Ÿæˆä¼ªSHA
                            'url': file_url,
                            'html_url': file_url,
                            'repository': {
                                'full_name': f"{owner}/{repo}",
                                'name': repo,
                                'owner': {'login': owner},
                                'pushed_at': None  # Webæœç´¢æ— æ³•è·å–è¿™ä¸ªä¿¡æ¯
                            }
                        }
                        
                        items.append(item)
                
                except Exception as e:
                    logger.debug(f"è§£æå•ä¸ªæœç´¢ç»“æœæ—¶å‡ºé”™: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"è§£ææœç´¢ç»“æœHTMLå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
        
        logger.info(f"ğŸ“‹ æˆåŠŸè§£æ {len(items)} ä¸ªæœç´¢ç»“æœ")
        return items
    
    def _estimate_total_count(self, html: str) -> int:
        """ä»HTMLä¸­ä¼°ç®—æœç´¢ç»“æœæ€»æ•°"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # æŸ¥æ‰¾æ˜¾ç¤ºç»“æœæ€»æ•°çš„å…ƒç´ 
            # GitHubé€šå¸¸åœ¨é¡µé¢é¡¶éƒ¨æ˜¾ç¤ºç±»ä¼¼ "123,456 results" çš„æ–‡æœ¬
            count_elem = soup.find('h3', string=lambda text: text and 'result' in text.lower())
            
            if count_elem:
                text = count_elem.get_text()
                # æå–æ•°å­—
                import re
                numbers = re.findall(r'[\d,]+', text)
                if numbers:
                    count_str = numbers[0].replace(',', '')
                    return int(count_str)
            
            # å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®æ•°å­—ï¼Œæ ¹æ®ç»“æœæ•°é‡ä¼°ç®—
            result_count = len(soup.find_all('div', {'class': 'code-list-item'}))
            if result_count > 0:
                return result_count * 10  # ç²—ç•¥ä¼°ç®—
        
        except Exception as e:
            logger.debug(f"ä¼°ç®—ç»“æœæ€»æ•°å¤±è´¥: {e}")
        
        return 100  # é»˜è®¤å€¼

    def get_file_content(self, item: Dict[str, Any]) -> Optional[str]:
        """
        è·å–æ–‡ä»¶å†…å®¹ï¼Œæ ¹æ®auth_modeé€‰æ‹©APIæˆ–Webæ–¹å¼
        
        Args:
            item: æ–‡ä»¶ä¿¡æ¯å­—å…¸
            
        Returns:
            æ–‡ä»¶å†…å®¹å­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å›None
        """
        if self.auth_mode == 'web':
            return self._get_file_content_web(item)
        else:
            return self._get_file_content_api(item)
    
    def _get_file_content_api(self, item: Dict[str, Any]) -> Optional[str]:
        """ä½¿ç”¨APIè·å–æ–‡ä»¶å†…å®¹ï¼ˆéœ€è¦tokenï¼‰"""
        repo_full_name = item["repository"]["full_name"]
        file_path = item["path"]

        metadata_url = f"https://api.github.com/repos/{repo_full_name}/contents/{file_path}"
        headers = {
            "Accept": "application/vnd.github.v3+json",
        }

        token = self._next_token()
        if token:
            headers["Authorization"] = f"token {token}"

        try:
            # è·å–proxyé…ç½®
            proxies = Config.get_random_proxy()

            logger.info(t('processing_file', metadata_url))
            if proxies:
                metadata_response = requests.get(metadata_url, headers=headers, proxies=proxies)
            else:
                metadata_response = requests.get(metadata_url, headers=headers)

            metadata_response.raise_for_status()
            file_metadata = metadata_response.json()

            # æ£€æŸ¥è¿”å›çš„æ˜¯å¦ä¸ºåˆ—è¡¨ï¼ˆç›®å½•å†…å®¹ï¼‰è€Œéå•ä¸ªæ–‡ä»¶
            if isinstance(file_metadata, list):
                logger.warning(t('unexpected_list_response', metadata_url))
                return None

            # æ£€æŸ¥æ˜¯å¦æœ‰base64ç¼–ç çš„å†…å®¹
            encoding = file_metadata.get("encoding")
            content = file_metadata.get("content")
            
            if encoding == "base64" and content:
                try:
                    # è§£ç base64å†…å®¹
                    decoded_content = base64.b64decode(content).decode('utf-8')
                    return decoded_content
                except Exception as e:
                    logger.warning(t('decode_failed', e))
            
            # å¦‚æœæ²¡æœ‰base64å†…å®¹æˆ–è§£ç å¤±è´¥ï¼Œä½¿ç”¨åŸæœ‰çš„download_urlé€»è¾‘
            download_url = file_metadata.get("download_url")
            if not download_url:
                logger.warning(t('no_download_url', metadata_url))
                return None

            if proxies:
                content_response = requests.get(download_url, headers=headers, proxies=proxies)
            else:
                content_response = requests.get(download_url, headers=headers)
            logger.info(t('checking_keys_from', download_url, content_response.status_code))
            content_response.raise_for_status()
            return content_response.text

        except requests.exceptions.HTTPError as e:
            status = e.response.status_code if e.response else None
            token_display = token[:20] if token else "None"
            
            # å°è¯•ä»å“åº”ä¸­æå–è¯¦ç»†é”™è¯¯ä¿¡æ¯
            error_message = "Unknown error"
            try:
                if e.response is not None:
                    error_json = e.response.json()
                    error_message = error_json.get('message', str(e))
                else:
                    error_message = str(e)
            except:
                error_message = str(e)
            
            # æ ¹æ®ä¸åŒçš„çŠ¶æ€ç æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            if status == 401:
                logger.error(t('token_invalid', token_display, error_message))
            elif status == 403:
                rate_limit_remaining = e.response.headers.get('X-RateLimit-Remaining', 'N/A')
                rate_limit_reset = e.response.headers.get('X-RateLimit-Reset', 'N/A')
                
                if rate_limit_reset != 'N/A':
                    try:
                        from datetime import datetime
                        reset_time = datetime.fromtimestamp(int(rate_limit_reset)).strftime('%Y-%m-%d %H:%M:%S')
                    except:
                        reset_time = rate_limit_reset
                else:
                    reset_time = 'N/A'
                
                if 'rate limit' in error_message.lower() or rate_limit_remaining == '0':
                    logger.warning(t('token_rate_limited', token_display, rate_limit_remaining, reset_time))
                else:
                    logger.error(t('token_forbidden', token_display, error_message))
            elif status == 429:
                rate_limit_remaining = e.response.headers.get('X-RateLimit-Remaining', '0')
                rate_limit_reset = e.response.headers.get('X-RateLimit-Reset', 'N/A')
                
                if rate_limit_reset != 'N/A':
                    try:
                        from datetime import datetime
                        reset_time = datetime.fromtimestamp(int(rate_limit_reset)).strftime('%Y-%m-%d %H:%M:%S')
                    except:
                        reset_time = rate_limit_reset
                else:
                    reset_time = 'N/A'
                
                logger.warning(t('token_rate_limited', token_display, rate_limit_remaining, reset_time))
            else:
                logger.error(t('token_error_detail', status or 'None', token_display, error_message))
            
            return None
        except requests.exceptions.RequestException as e:
            logger.error(t('fetch_file_failed', metadata_url, type(e).__name__))
            return None
    
    def _get_file_content_web(self, item: Dict[str, Any]) -> Optional[str]:
        """ä½¿ç”¨Webæ–¹å¼ç›´æ¥è·å–rawæ–‡ä»¶å†…å®¹ï¼ˆåŸºäºcookieè®¤è¯ï¼‰"""
        try:
            # è½®æ¢session cookieï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼‰
            if self.github_sessions and len(self.github_sessions) > 1:
                current_session = self._next_session()
                self._set_session_cookie(current_session)
            
            repo_full_name = item["repository"]["full_name"]
            file_path = item["path"]
            
            # æ„å»ºraw URL
            # æ ¼å¼: https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{path}
            # ç”±äºä»webæœç´¢æ— æ³•ç›´æ¥è·å–branchï¼Œæˆ‘ä»¬å°è¯•å¸¸è§çš„åˆ†æ”¯å
            branches_to_try = ['main', 'master', 'develop']
            
            # å¦‚æœhtml_urlä¸­åŒ…å«blobä¿¡æ¯ï¼Œå°è¯•ä»ä¸­æå–branch
            html_url = item.get('html_url', '')
            if '/blob/' in html_url:
                parts = html_url.split('/blob/')
                if len(parts) > 1:
                    branch_and_path = parts[1].split('/', 1)
                    if branch_and_path:
                        branches_to_try.insert(0, branch_and_path[0])
            
            proxies = Config.get_random_proxy()
            
            for branch in branches_to_try:
                raw_url = f"https://raw.githubusercontent.com/{repo_full_name}/{branch}/{file_path}"
                
                try:
                    logger.info(f"ğŸŒ è·å–æ–‡ä»¶å†…å®¹: {raw_url}")
                    
                    if proxies:
                        response = self.session.get(raw_url, timeout=30, proxies=proxies)
                    else:
                        response = self.session.get(raw_url, timeout=30)
                    
                    if response.status_code == 200:
                        logger.info(f"âœ… æˆåŠŸè·å–æ–‡ä»¶å†…å®¹ (branch={branch})")
                        return response.text
                    elif response.status_code == 404:
                        # åˆ†æ”¯ä¸å­˜åœ¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ª
                        continue
                    else:
                        response.raise_for_status()
                
                except requests.exceptions.RequestException:
                    # å°è¯•ä¸‹ä¸€ä¸ªåˆ†æ”¯
                    continue
            
            logger.warning(f"âš ï¸ æ— æ³•è·å–æ–‡ä»¶å†…å®¹: {repo_full_name}/{file_path} (å°è¯•äº†åˆ†æ”¯: {branches_to_try})")
            return None
        
        except Exception as e:
            logger.error(f"âŒ è·å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            return None

    @staticmethod
    def create_instance(tokens: List[str], auth_mode: str = None, github_sessions: List[str] = None) -> 'GitHubClient':
        """
        åˆ›å»ºGitHubClientå®ä¾‹
        
        Args:
            tokens: GitHub Tokenåˆ—è¡¨
            auth_mode: è®¤è¯æ¨¡å¼ï¼Œå¦‚æœä¸ºNoneåˆ™ä»Configè¯»å–
            github_sessions: GitHub session cookieåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä»Configè¯»å–
            
        Returns:
            GitHubClientå®ä¾‹
        """
        if auth_mode is None:
            auth_mode = Config.GITHUB_AUTH_MODE
        if github_sessions is None:
            github_sessions = Config.GITHUB_SESSIONS
        return GitHubClient(tokens, auth_mode, github_sessions)
